---
title: "PyTorch-Based Neural Network Approximation of ODE Solutions"
author: Yue Wu^[yue.wu@uga.edu], Michael Judge, Jonathan Arnold, Arthur Edison, Shannon Quinn, and Heinz-Bernd Schuttler
date: 06 15, 2020
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    self_contained: true
params:
  n: 100
bibliography: /Users/yuewu/Dropbox (Edison_Lab@UGA)/Projects/Bioinformatics_modeling/Documents/abstract/scipy2020/reference/scipy2020.ris
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---

```{r setup, include=FALSE}
#eval=FALSE echo=FALSE include=FALSE
knitr::opts_chunk$set(echo = TRUE)
```

# Background

Ordinary differential equation (ODE) -based simulation is a popular choice for modeling real-world systems, including metabolic networks, gene regulatory systems and epidemiology [@273; @255; @457]. For the model to be representative, an inverse problem needs to be solved: estimating unknown parameters from experimental measurements. While an ODE model can often capture the real-world dynamics, solving the inverse problem is hard, involving considerable amounts of ODE solutions and nonlinear optimization [@273].

Inference of model parameters is often a nonlinear optimization problem, comparing model predictions with measurements, by way of an objective function, and searching for better fitting model parameters (Figure 1.1). This process requires considerable iterations of producing and evaluating different models. A simplified model of central metabolism involves 32 equations and ~200 parameters and is fit with ~1000 data points. A Markov chain Monte Carlo (MCMC) ( \@ref(faq1) ) algorithm [@273] needs around $10^7$  ODE solutions. Even with a modern ODE solver (Adaptive Backward Differentiation), it still takes about a week to finish. When the parameter set is stiff, considerably larger, if not impossible, simulation time is needed. Previously, we constructed a program to systematically ignore stiff parameter sets ([pearc2019](https://www.dropbox.com/s/i3ln8kkn5efy7nv/poster.final3.pdf?dl=0)). This reduces the simulation time cost, while at the same time unavoidably introduces biases.
  
```{r diagram-invserseprob,fig.show="hold",fig.align='center', fig.dim=c(2,4),out.width="100%",echo=FALSE}
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = LR]
  
  node [shape = rectangle]
  rec1 [label = 'unknown model']
  rec2 [label = 'measurements']
  rec3 [label = 'assumed model']
  rec4 [label = 'prediction']
  rec5 [label = 'objective function']

  # edge definitions with the node IDs
  rec1 -> rec2 [label = 'produce']
  rec3 -> rec4 [label = 'ODE solver']
  rec2 -> rec5
  rec4 -> rec5
  }",
  height = 100)
```
Figure 1.1: Inverse problem diagram. The goal is to find a model (in topology and parameter) that can approximate the unknown generating model. The measurements are produced from an unknown underlying model and random noises are also involved. By comparing predictions with the measurements, the assumed model can be improved. Such comparison are often implemented through an objective function, which evaluates parameters in the assumed model.

  
While ODE solvers have variable time costs, neural networks have near constant cost, which is also often small. There has been success in simulating dynamic systems by neural networks [@466; @458; @465], but a high dimensional ODE initial value problem has not been carefully studied yet. In an ODE initial value problem, the input is the parameters of the assumed model and the output is the time dynamics. This is often the inner loop of the inverse problem. In this project, we explored the possibility of replacing ODE solver with trained neural networks to solve initial value problem.

# Methods

Dynamics ($Y(t)$) of an ODE system solely depends on initial conditions ($Y_{ini}$), model parameters ($\theta$), and time ($t$). In both ODE solver and neural network, we can treat the vector $X=[\theta, Y_{ini}, t]$ as the input and the vector $Y(t)$ as the output. The solution of such a system by ODE solver can produce pairs $[X, Y(t)]$ as training data.

## Neural network architecture

ResNet has been a popular structure in multiple tasks, including image classification, and it simplifies the training process by identity matching [@450]. In our modified architecture, we replaced the convolutional layers with linear layers. For example, [resnet18_mlp](https://github.com/artedison/NeuralSimODE/blob/fc5699f8962087fef1c74da62969f742bf96c288/src/nnt_struc.py#L458) is similar to original resnet18 [@450], except that we used [nn.Linear](https://github.com/artedison/NeuralSimODE/blob/fc5699f8962087fef1c74da62969f742bf96c288/src/nnt_struc.py#L27) to replace [nn.Conv2d](https://github.com/pytorch/vision/blob/c2e8a00885e68ae1200eb6440f540e181d9125de/torchvision/models/resnet.py#L35) (see [here](https://github.com/artedison/NeuralSimODE/blob/fc5699f8962087fef1c74da62969f742bf96c288/src/nnt_struc.py) and following code block). The hidden layer size was controlled by its ratio ($R$) to the input layer size so, network complexity was automatically adjusted in dynamic systems of different sizes. Batch normalization and RELU (rectified linear unit) were used in the architecture [@464]. Adam was used as the optimizer [@463]. Hyperparameter tuning was done both manually and by Optuna [@462]. Mean squared error (MSE) was the objective function. 20% of samples are left out for testing. Besides ResNet, multilayer perceptron (MLP) and recurrent structures were also tested. The neural network was implemented in [PyTorch](https://pytorch.org) and shared in [GitHub](https://github.com/artedison/NeuralSimODE/tree/master/src).

```{python codeblock, eval=FALSE, fig.cap=""}
class BasicBlock(nn.Module):
    expansion=1
    __constants__=['downsample']

    def __init__(self,inplanes,planes,downsample=None,groups=1,
                 base_width=64,norm_layer=None,p=0.0):
        # inplanes: input size
        # planes: internal size
        # downsample: whehter downsample the idnetify mapping. default: None
        # groups: was used for "Aggregated Residual Transformation" (not used currently) Defualt 1
        # width_per_group: used for "Wide Residual Networks" and "Aggregated Residual Transformation" Defualt 64
        # norm_layer: used to specify batch normalization function. Default None
        # p: dropbout probability Default 0.0
        super(BasicBlock,self).__init__()
        if norm_layer is None:
            norm_layer=nn.BatchNorm1d
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        self.fc1=line1d(inplanes,planes)
        self.bn1=norm_layer(planes)
        self.relu=nn.ReLU(inplace=True)
        self.fc2=line1d(planes,inplanes)
        self.bn2=norm_layer(inplanes)
        self.downsample=downsample
        self.p=p

    def forward(self,x):
        identity=x
        out=F.dropout(self.fc1(x),training=self.training,p=self.p)
        out=self.bn1(out)
        out=self.relu(out)
        out=F.dropout(self.fc2(out),training=self.training,p=self.p)
        out=self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out=self.relu(out)
        return out
```

The model is trained on P100 GPUs of both sapelo2 at [GACRC](https://gacrc.uga.edu) and PSC Bridges at [XSEDE](https://www.xsede.org).

## Training data simulation {#training}

We simulated dynamic systems of different complexities as training samples. The coupled linear ODE system is simulated with different dimensions (4-32) (\@ref(tab:table1)), fixed topologies, and 10000 random generated parameter sets. We shifted the eigen value to be negative in real part to resemble most real-world systems. For linear ODE, we produced analytic solutions without ODE solver and list detailed procedures in \@ref(faq2).

```{r table1,echo=FALSE, results='asis'}
dim=c(2,3,4,8,12,16)
ntheta=c(3,5,7,15,27,37)*2
ny=c(4,6,8,16,24,32)
ninput=c(11,17,23,47,79,107)
noutput=c(4,6,8,16,24,32)
dim_tab=data.frame("dimension of system"=dim,"number of theta"=ntheta,"number of Ys"=ny,"number of inputs"=ninput,"number of outputs"=noutput)
knitr::kable(dim_tab,caption="Dimensions of simulated data set. The equations are simualted in complex number so, here number of Ys (real and imaginary) are double the dimension of the system. Refer to \\@ref(faq2) for details in simulation",col.names=c("dimension of the system","number of theta","number of Ys","number of inputs","number of outputs"))
```

# Results

We have successfully approximated solutions for low dimensional ODE systems. For ODE systems with six equations, the neural network can achieve MSE of 0.2 for both training and testing sets after 500 epochs (Figure 3.1). Some representative cases of training and testing samples are presented in Figure 3.2. As the performance is similar for training and testing sets, further training and hyperparameter tuning can probably improve the results. From Figure 3.2, we can also see that different dynamics can be approximated. Similar performance can also be achieved by MLP.

<center>

![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/Projects/Bioinformatics_modeling/nc_model/nnt/simulation/simulat.linear.small.corr/result/res1/mse_epoch.png){#id .class width=70% height=70%}

</center>
Figure 3.1: MSE through epochs for multiple training settings. Preliminary  training results are presented. Solid (dotted) line represent the MSE on training (testing) set. The best models is based on ResNet18, have hidden layer size eight times that of the input layer and use Adam optimizer. Details on training process can be found in Methods \@ref(training).


![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/Projects/Bioinformatics_modeling/package.formulate/NeuralSimODE/docs/scipy2020/figure3.2.png){#id .class width=100% height=100%}

Figure 3.2: Fitting results of the preliminary trained model. The first (second) row is the model performance in the training (testing) set. X-axis is time and Y-axis is normalized $Y$. Two cases are presented for both training and testing set. The blue curve is the ground truth (exact ODE solution) and the orange curve is neural network estimation. Details in neural network training can be found in Methods \@ref(training).


The neural network performances degrades with higher ODE system dimensions. Specifically, for ODE systems with 32 equations, the model performance is still not acceptable after extensive hyperparameter tuning (including by Optuna [@462]) and trying different technical options. Multiple options of scheduler, recurrent architectures, and optimizer were tried, as well as using larger training sample. Among these trials, [ReduceLROnPlateau](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau) scheduler, [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) optimizer, and larger sample size helped to improve performance.

# Next steps

1. Training the neural network on larger data sets. Efficient approaches are needed for generating and training large data sets. Both processes will need parallelization. A separate data loading process is also necessary because of the memory cost.

2. Data augmentation is needed to promote training. A novel transformation of time dynamics has been implemented and is currently being tested.

# FAQ

## Why is MCMC used instead of other nonlinear optimizer? {#faq1}

In the ensemble simulation procedure [@273], MCMC converges to a local fit region in the model parameter space. This local region contains many parameter sets (the model ensemble) and each set in the MCMC sample is of similar fit quality, given the experimental data. An ensemble of similar fitted models gives a natural uncertainty estimation for the fitting. This is particularly crucial when only sparse less experimental data are available for a model of high complexity.

## The detailed procedure for simulating linear ODE {#faq2}

The procedure is to compute the ODE $\frac{dY(t)}{dt}=HY(t)$ and format the input and output for neural network. There is $N$ dimensions (equations of complex number) and $M$ time points.

a. Random select non-zero elements (NE) for $H$ matrix. Diagonal elements are always included and they represent the contribution of a value to its own derivative. This topology (location of non-zero elements) is fixed for the following simulations and recorded in $I_{conn}$

b. Generate $\widetilde{H}$

    For each NE, sample real and imaginary parts from $Unif(-1,1)$
    
c. Eigen value shifting

    Decomposition $\widetilde{H}=U'D'V'$ and $d'=diag(D')$
    
    $\Delta{d'}=|max(real(d'))|$
    
    $H=\widetilde{H}-S \Delta d' I$ where $S=1.01$ to make sure all eigen values have negative real parts and so the dynamic system does not diverge.
    
    Decomposition $H=UDV$
    
d. Generate $Y_{ini}$

    Sample real and imaginary parts from $Unif(-1,1)$

e. Generate time series for time grid $\hat{t}=[t_1, ..., t_M]$
    
    $A=VY_{ini}$
    
    $B=[u_1 a_1, ... u_k a_k, ..., u_Na_N]=U \ diag(A)$, $u_k$ is the k-th column vector of U
    
    $E=[e^{\hat{d} \hat{t}} ]$ where $\hat{d}=diag(D)$
    
    $Y(t)=BE$
    
f. Rescale time series

    For every equation ($n$) in $N$, the normalized result is
    
    $\widetilde{Y_n(t_m)} = \frac{Y_n(t_m)}{\sqrt{\Omega_{n}}}$ where $\Omega_{n}=\sum^{M}_{m=1}{|Y_n(t_m)|^2}$
    
g. Formulate input and output for neural network

    Input: $[Re(H(I_{conn})), Im(H(I_{conn})), Re(Y_{ini}), Im(Y_{ini}),\hat{t}]$
    
    Output: $[Re(Y(t)), Im(Y(t)]$

# Aknowledgement

This is work is supported by NSF MCB-1713746. We thank [GACRC](https://gacrc.uga.edu) and [XSEDE](https://www.xsede.org) for computational resources and technical supports. XSEDE is supported supported by National Science Foundation grant number ACI-1548562. Our start-up grant in XSEDE(TG-MCB180198) use GPU resources in PSC Bridge. We thank the Georgia Research Alliance, the Institute of Bioinformatics, and the Complex Carbohydrate Research Center for supporting this work.

![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/yuewu_local/icons/ccrc.png){#id .class width=20% height=20%}
![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/yuewu_local/icons/gacrc.png){#id .class width=20% height=20%}
![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/yuewu_local/icons/gra.png){#id .class width=20% height=20%}
![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/yuewu_local/icons/iob.png){#id .class width=20% height=20%}
![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/yuewu_local/icons/nsf.png){#id .class width=10% height=10%}
![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/yuewu_local/icons/uga.png){#id .class width=10% height=10%}
![](/Users/yuewu/Dropbox (Edison_Lab@UGA)/yuewu_local/icons/xsede.png){#id .class width=20% height=20%}

# Reference
